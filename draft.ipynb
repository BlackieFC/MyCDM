{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检查嵌入分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "arr = np.load('data/NIPS34/all/exer_embeds.npy')\n",
    "print(arr.shape)\n",
    "\n",
    "# 计算每一行的L2范数\n",
    "row_norms = np.linalg.norm(arr, axis=1)\n",
    "\n",
    "# 绘制直方图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(row_norms, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Row-wise L2 Norm Distribution')\n",
    "plt.xlabel('Norm Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 返回计算结果（前5行示例）\n",
    "print(\"前5行的范数值：\")\n",
    "print(row_norms[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将csv格式的数据转换为NCDM标准输入格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "\n",
    "\n",
    "# 指定CSV文件的路径\n",
    "# senario = 'Algebra'\n",
    "# senario = 'GeometryandMeasure'\n",
    "# senario = 'Number'\n",
    "senario = 'Algebra_cold'\n",
    "\n",
    "# file_name = 'train'\n",
    "# file_name = 'val'\n",
    "file_name = 'test'\n",
    "\n",
    "root_dir = f'data/NIPS34/{senario}'\n",
    "file_in = f'{root_dir}/{file_name}.csv'\n",
    "file_out = f'{root_dir}/{file_name}.json'\n",
    "\n",
    "# 使用pandas的read_csv方法读取CSV文件\n",
    "df = pd.read_csv(file_in)\n",
    "# 重命名指定的列\n",
    "df.rename(columns={'QuestionId':'exer_id', 'UserId':'user_id', 'IsCorrect':'score', 'Kc':'knowledge_code'}, inplace=True)\n",
    "# 删除多余的列\n",
    "df.drop(columns=['Time', 'AnswerValue', 'CorrectAnswer', 'AnswerId'], inplace=True)\n",
    "# 将每一行转换为字典，并存储在列表中\n",
    "dict_list = df.to_dict(orient='records')\n",
    "\n",
    "# 格式规范化\n",
    "for elem in dict_list:\n",
    "    elem['knowledge_code'] = ast.literal_eval(elem['knowledge_code'])\n",
    "\n",
    "# 保存为json\n",
    "with open(file_out, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(dict_list, json_file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逆操作：从json转csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "# 指定场景参数\n",
    "senario = 'longtail'\n",
    "\n",
    "# file_name = 'train'\n",
    "file_name = 'test'\n",
    "\n",
    "root_dir = f'data/NIPS34/{senario}'\n",
    "file_in = f'{root_dir}/{file_name}.json'\n",
    "file_out = f'{root_dir}/{file_name}.csv'\n",
    "\n",
    "# 读取JSON文件\n",
    "with open(file_in, 'r', encoding='utf-8') as f:\n",
    "    dict_list = json.load(f)\n",
    "\n",
    "# 转换为DataFrame\n",
    "df = pd.DataFrame(dict_list)\n",
    "\n",
    "# 将列表转换回字符串格式\n",
    "df['knowledge_code'] = df['knowledge_code'].astype(str)\n",
    "\n",
    "# 列名逆向映射恢复\n",
    "df.rename(columns={\n",
    "    'exer_id': 'item_id',\n",
    "    'user_id': 'user_id',\n",
    "    'score': 'score'\n",
    "    # 'knowledge_code': 'Kc'\n",
    "}, inplace=True)\n",
    "\n",
    "# # 添加原始被删除的列（用空值填充）\n",
    "# for col in ['Time', 'AnswerValue', 'CorrectAnswer', 'AnswerId']:\n",
    "#     df[col] = pd.NA  # 使用pandas的缺失值标记\n",
    "\n",
    "# 按原始列顺序排序（假设原始列顺序如下）\n",
    "column_order = [\n",
    "    'user_id', \n",
    "    'item_id', \n",
    "    'score'\n",
    "]\n",
    "df = df[column_order]\n",
    "\n",
    "# 保存为CSV\n",
    "df.to_csv(file_out, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计高频和长尾KC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[高频题目] 出现超过10次的QuestionId：594个\n",
      "[199, 911, 625, 855, 520, 83, 528, 547, 533, 676, 856, 460, 50, 815, 47, 727, 178, 761, 502, 634, 599, 836, 494, 421, 312, 91, 22, 670, 639, 449, 391, 862, 409, 844, 342, 635, 941, 209, 463, 185, 290, 939, 236, 943, 446, 372, 283, 527, 749, 887, 337, 592, 638, 461, 293, 134, 831, 596, 637, 40, 813, 278, 265, 295, 45, 583, 614, 525, 664, 75, 745, 605, 333, 349, 311, 49, 39, 56, 101, 263, 885, 190, 587, 84, 360, 183, 335, 282, 457, 195, 626, 150, 868, 325, 585, 138, 850, 148, 808, 184, 417, 118, 249, 177, 923, 210, 38, 858, 297, 740, 751, 830, 790, 119, 8, 601, 932, 52, 383, 90, 212, 475, 804, 673, 82, 889, 732, 644, 834, 480, 522, 472, 787, 260, 852, 376, 945, 92, 629, 292, 193, 133, 882, 506, 451, 789, 53, 186, 24, 495, 154, 924, 767, 129, 645, 213, 816, 896, 439, 34, 611, 76, 361, 368, 402, 31, 16, 539, 707, 559, 88, 624, 615, 435, 304, 37, 211, 493, 800, 365, 327, 328, 158, 145, 554, 805, 369, 160, 423, 579, 908, 485, 403, 392, 147, 513, 86, 838, 116, 64, 120, 509, 436, 795, 206, 628, 863, 507, 755, 674, 197, 465, 231, 503, 772, 691, 202, 617, 710, 298, 627, 406, 496, 54, 18, 310, 289, 122, 279, 261, 171, 492, 413, 207, 653, 898, 343, 589, 729, 920, 683, 201, 241, 709, 501, 106, 226, 326, 307, 806, 468, 622, 384, 837, 498, 748, 204, 396, 456, 237, 267, 829, 523, 588, 251, 682, 66, 157, 811, 452, 14, 474, 79, 535, 514, 840, 29, 690, 688, 717, 927, 823, 4, 555, 424, 636, 351, 612, 262, 661, 766, 12, 600, 657, 155, 870, 434, 259, 907, 227, 264, 770, 78, 731, 105, 20, 700, 750, 530, 586, 277, 508, 935, 577, 560, 667, 85, 488, 792, 571, 891, 130, 776, 341, 218, 641, 684, 364, 162, 711, 536, 10, 877, 609, 291, 734, 222, 491, 716, 916, 685, 152, 581, 256, 385, 380, 814, 440, 317, 880, 572, 26, 594, 933, 483, 467, 878, 662, 487, 550, 835, 564, 737, 137, 97, 13, 124, 350, 431, 404, 200, 280, 125, 306, 63, 414, 176, 359, 490, 534, 110, 845, 818, 489, 871, 437, 174, 398, 872, 801, 302, 275, 725, 354, 247, 353, 191, 576, 375, 778, 415, 822, 582, 575, 848, 476, 386, 537, 288, 5, 358, 510, 301, 74, 17, 215, 620, 132, 36, 853, 9, 705, 756, 515, 308, 316, 378, 330, 430, 723, 741, 41, 221, 561, 681, 910, 411, 573, 631, 865, 652, 720, 869, 172, 466, 324, 866, 917, 229, 232, 286, 697, 321, 181, 268, 874, 455, 339, 334, 23, 542, 595, 355, 151, 647, 258, 713, 569, 901, 198, 779, 482, 780, 377, 272, 532, 900, 320, 43, 944, 168, 516, 607, 454, 395, 35, 857, 458, 925, 356, 188, 42, 102, 471, 793, 399, 98, 864, 702, 329, 336, 743, 121, 443, 381, 929, 189, 883, 619, 807, 140, 796, 450, 309, 568, 422, 144, 173, 632, 531, 366, 504, 649, 401, 266, 556, 693, 544, 799, 797, 164, 109, 240, 763, 752, 728, 149, 538, 28, 115, 418, 937, 936, 400, 899, 407, 651, 524, 708, 444, 192, 426, 303, 812, 719, 593, 340, 765, 447, 666, 107, 179, 557, 648, 374, 861, 909, 775, 892, 578, 425, 876, 843, 208, 68, 318, 703, 893, 442, 821, 180, 694, 724, 481, 126, 817, 58, 886, 136, 771, 774, 419, 61, 842]\n",
      "\n",
      "[低频题目] 出现不超过3次的QuestionId：332个\n",
      "[921, 706, 655, 613, 159, 602, 234, 71, 196, 584, 479, 69, 228, 567, 80, 553, 675, 300, 759, 100, 344, 163, 60, 305, 257, 922, 390, 698, 225, 918, 128, 141, 131, 671, 175, 621, 296, 851, 773, 7, 633, 73, 245, 96, 429, 19, 21, 608, 757, 928, 730, 603, 832, 153, 905, 1, 847, 117, 630, 860, 597, 108, 254, 270, 888, 271, 322, 104, 67, 598, 203, 112, 562, 833, 127, 839, 276, 32, 782, 784, 884, 114, 511, 739, 512, 580, 783, 99, 940, 895, 521, 253, 408, 382, 224, 942, 930, 692, 946, 747, 405, 352, 55, 517, 646, 348, 926, 938, 642, 658, 859, 819, 216, 764, 156, 754, 441, 873, 828, 902, 182, 123, 820, 142, 726, 546, 712, 610, 640, 663, 826, 338, 72, 478, 679, 650, 166, 798, 672, 30, 809, 217, 846, 89, 269, 3, 791, 802, 219, 464, 788, 762, 669, 604, 473, 48, 867, 794, 57, 111, 825, 545, 214, 103, 704, 416, 469, 500, 526, 323, 285, 686, 373, 904, 281, 459, 0, 529, 931, 934, 758, 563, 497, 389, 412, 420, 274, 768, 505, 875, 62, 659, 252, 827, 744, 919, 519, 255, 371, 558, 6, 689, 462, 453, 93, 484, 220, 894, 432, 738, 46, 590, 135, 551, 591, 677, 246, 243, 654, 854, 643, 223, 445, 11, 273, 543, 233, 695, 51, 541, 540, 25, 319, 77, 736, 549, 777, 250, 803, 230, 769, 890, 696, 687, 913, 906, 699, 187, 70, 248, 947, 616, 362, 618, 915, 665, 781, 746, 170, 394, 849, 167, 94, 370, 701, 714, 146, 824, 65, 566, 59, 565, 735, 656, 165, 623, 388, 448, 914, 314, 346, 379, 486, 363, 242, 552, 721, 903, 95, 470, 161, 332, 718, 44, 87, 786, 742, 331, 235, 2, 438, 367, 410, 27, 753, 785, 433, 347, 879, 668, 570, 678, 194, 143, 169, 733, 912, 387, 574, 284, 205, 238, 881, 81, 33, 294, 841, 810, 345, 139, 499, 15]\n",
      "高频数据已保存至：data/NIPS34/longtail/test_highfreq.csv（共 180058 行）\n",
      "低频数据已保存至：data/NIPS34/longtail/test_longtail.csv（共 96452 行）\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 文件路径\n",
    "senario = 'longtail'\n",
    "file_name = 'test'\n",
    "root_dir = f'data/NIPS34/{senario}'\n",
    "csv_train = f'{root_dir}/train.csv'\n",
    "csv_test = f'{root_dir}/{file_name}.csv'\n",
    "output_high = f'{root_dir}/{file_name}_highfreq.csv'  # 高频结果文件\n",
    "output_low = f'{root_dir}/{file_name}_longtail.csv'   # 低频结果文件\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(csv_train)\n",
    "df_test = pd.read_csv(csv_test)\n",
    "\n",
    "# 统计QuestionId出现次数\n",
    "question_counts = df['item_id'].value_counts()\n",
    "\n",
    "# 获取不同频次的题目ID列表\n",
    "high_freq = question_counts[question_counts > 10].index.tolist()   # 计数>10的ID\n",
    "low_freq = question_counts[question_counts <= 3].index.tolist()    # 计数≤3的ID\n",
    "\n",
    "print(f'[高频题目] 出现超过10次的QuestionId：{len(high_freq)}个')\n",
    "print(high_freq)\n",
    "\n",
    "print(f'\\n[低频题目] 出现不超过3次的QuestionId：{len(low_freq)}个')\n",
    "print(low_freq)\n",
    "\n",
    "# 提取高频题目数据\n",
    "high_df = df_test[df_test['item_id'].isin(high_freq)]\n",
    "# 提取低频题目数据\n",
    "low_df = df_test[df_test['item_id'].isin(low_freq)]\n",
    "\n",
    "# 保存结果（保留原始列结构）\n",
    "high_df.to_csv(output_high, index=False)\n",
    "low_df.to_csv(output_low, index=False)\n",
    "\n",
    "print(f'高频数据已保存至：{output_high}（共 {len(high_df)} 行）')\n",
    "print(f'低频数据已保存至：{output_low}（共 {len(low_df)} 行）')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check冷启动情景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 文件路径\n",
    "senario = 'student_all'\n",
    "root_dir = f'data/NIPS34/{senario}'\n",
    "csv_train = f'{root_dir}/train.csv'\n",
    "csv_test = f'{root_dir}/test.csv'\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(csv_train)\n",
    "df_test = pd.read_csv(csv_test)\n",
    "\n",
    "# 统计QuestionId出现次数\n",
    "question_counts = df['QuestionId'].value_counts()\n",
    "# 获取不同频次的题目ID列表\n",
    "pid_train = question_counts.index.tolist()   # 计数>10的ID\n",
    "\n",
    "# 统计QuestionId出现次数\n",
    "question_counts = df_test['QuestionId'].value_counts()\n",
    "# 获取不同频次的题目ID列表\n",
    "pid_test = question_counts.index.tolist()   # 计数>10的ID\n",
    "\n",
    "print(set(pid_train) & set(pid_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel\n",
    "import numpy as np\n",
    "from utils.loss import cl_and_reg_loss\n",
    "\n",
    "\n",
    "class MyCDM_MLP_FFT(nn.Module):\n",
    "    \"\"\"\n",
    "    使用全量微调BERT，需要把题目文本组织为字典形式的分词结果作为BERT输入\n",
    "        # 全量微调模式下，所有BERT参数都可以训练，不需要冻结参数\n",
    "        # 默认情况下，所有参数都已经是requires_grad=True的状态\n",
    "    \"\"\"\n",
    "    def __init__(self, num_students, bert_model_name='bert-base-uncased', tau=0.1, lambda_reg=1.0, lambda_cl=0.5):\n",
    "        super().__init__()\n",
    "        self.tau = tau\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.lambda_cl = lambda_cl\n",
    "\n",
    "        # 读取预训练BERT\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.d_model = self.bert.config.hidden_size\n",
    "        # 学生能力嵌入层（u+和u-）\n",
    "        self.stu_pos = nn.Embedding(\n",
    "            num_embeddings=num_students,\n",
    "            embedding_dim=self.d_model\n",
    "        )\n",
    "        self.stu_neg = nn.Embedding(\n",
    "            num_embeddings=num_students,\n",
    "            embedding_dim=self.d_model\n",
    "        )\n",
    "        # MLP预测头\n",
    "        self.prednet = nn.Sequential(\n",
    "            nn.Linear(3 * self.d_model, 2 * self.d_model),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(2 * self.d_model, self.d_model),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.d_model, 1)\n",
    "        )\n",
    "        # 初始化参数\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"参数初始化\"\"\"\n",
    "        nn.init.normal_(self.stu_pos.weight, mean=0.0, std=0.1)\n",
    "        nn.init.normal_(self.stu_neg.weight, mean=0.0, std=0.1)\n",
    "        # self.prednet\n",
    "        for module in self.prednet:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, stu_ids, exer_in):\n",
    "        # 学生的正负双模态表征\n",
    "        u_pos = self.stu_pos(stu_ids)      # [batch_size, 768]\n",
    "        u_neg = self.stu_neg(stu_ids)      # [batch_size, 768]\n",
    "        # 题目表征\n",
    "        bert_output = self.bert(           # [batch_size, 768]，提取CLS token作为题目嵌入\n",
    "            input_ids=exer_in[\"input_ids\"],\n",
    "            attention_mask=exer_in[\"attention_mask\"])\n",
    "        exer_emb = bert_output.last_hidden_state[:, 0, :]\n",
    "\n",
    "        \"\"\"MLP预测头（1）不与问题交互\"\"\"\n",
    "        # stu_emb = u_pos - u_neg                                       # [batch_size, 768]\n",
    "        # logits = self.prednet(torch.cat([exer_emb, stu_emb], dim=1))  # [batch_size, 1]\n",
    "        \"\"\"MLP预测头（2）与问题交互\"\"\"\n",
    "        logits = self.prednet(torch.cat([exer_emb, torch.multiply(exer_emb, u_pos), torch.multiply(exer_emb, u_neg)], dim=1))  # [batch_size, 1]\n",
    "        output = torch.sigmoid(logits).squeeze(-1)                      # [batch_size]\n",
    "\n",
    "        return output, exer_emb, u_pos, u_neg\n",
    "\n",
    "    def get_loss(self, output, labels):\n",
    "        \"\"\"计算总损失\"\"\"\n",
    "        preds, exer_emb, u_pos, u_neg = output\n",
    "        # BCE损失 <=> 预测损失\n",
    "        bce_loss = nn.BCELoss(reduction='mean')(preds, labels.squeeze())  # [batch_size]\n",
    "        # 对比损失 & 正则化项\n",
    "        loss_contrast, loss_reg = cl_and_reg_loss(exer_emb, u_pos, u_neg, labels, self.tau, delta=0.1, norm=True)\n",
    "        # 总损失\n",
    "        if self.lambda_cl < 1:\n",
    "            total_loss = (1-self.lambda_cl)*bce_loss + self.lambda_cl*loss_contrast + self.lambda_reg*loss_reg\n",
    "        else:\n",
    "            total_loss = bce_loss + self.lambda_cl*loss_contrast + self.lambda_reg*loss_reg\n",
    "        return total_loss, bce_loss, loss_contrast, loss_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.load_data import MyDataloader\n",
    "from models.model import Baseline_IRT, Baseline_MLP, MyCDM_MLP, IRT, MyCDM_MSA, MyCDM_IRT\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    解析命令行参数\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description='模型训练参数配置')\n",
    "\n",
    "    # 实验配置\n",
    "    parser.add_argument('--mode', choices=['baseline', 'freeze', 'fine-tune'], default='freeze', help='实验模式')\n",
    "    parser.add_argument('--proj_name', type=str, default='freeze_250221_00', help='项目名称，用于保存检查点')\n",
    "    parser.add_argument('--data', type=str, default='NIPS34', choices=['NIPS34'], help='使用的数据集名称')\n",
    "    parser.add_argument('--scenario', type=str, default='all', choices=['all', 'Algebra', 'Algebra_cold', 'GeometryandMeasure', 'Number'], help='情景')\n",
    "\n",
    "    # 训练超参数\n",
    "    parser.add_argument('--bs', type=int, default=256, help='批次大小')\n",
    "    parser.add_argument('--epoch', type=int, default=100, help='最大训练轮数')\n",
    "    parser.add_argument('-lr', '--learning_rate', type=float, default=0.001, help='学习率')\n",
    "\n",
    "    # 模型配置\n",
    "    parser.add_argument('--bert_path', type=str, help='BERT预训练模型路径',\n",
    "                        default='/mnt/new_pfs/liming_team/auroraX/songchentao/llama/bert-base-uncased')\n",
    "    parser.add_argument('--tau', type=float, default=0.1, help='温度系数')\n",
    "    parser.add_argument('--lambda_cl', type=int, default=0.5, help='对比损失权重')\n",
    "    parser.add_argument('--lambda_reg', type=int, default=0.1, help='正则损失权重')\n",
    "\n",
    "    # 训练控制\n",
    "    parser.add_argument('--grid_search', action='store_true', help='格点搜索调参')\n",
    "    parser.add_argument('-esp', '--early_stop_patience', type=int, default=10, help='早停等待轮数')\n",
    "    parser.add_argument('-ckpt', '--checkpoint_dir', type=str, default=None, help='检查点保存目录 (默认: ../checkpoints/{proj_name})')\n",
    "    parser.add_argument('--verbose', type=int, default=0, help='是否显示epoch内进度')\n",
    "\n",
    "    _args = parser.parse_args()\n",
    "\n",
    "    # 后处理依赖参数\n",
    "    if _args.checkpoint_dir is None:\n",
    "        _args.checkpoint_dir = f'../checkpoints/{_args.proj_name}'\n",
    "\n",
    "    return _args\n",
    "\n",
    "\n",
    "def train(_model, _train_loader, _optimizer, _device, mode='baseline', verbose=0):\n",
    "    \"\"\"\n",
    "    模型训练函数\n",
    "        mode=['baseline','freeze','fine-tune']\n",
    "    \"\"\"\n",
    "    _model.train()\n",
    "    total_loss = 0.0\n",
    "    pred_loss = 0.0\n",
    "    cl_loss = 0.0\n",
    "    reg_loss = 0.0\n",
    "\n",
    "    count = 0\n",
    "    for batch in _train_loader:\n",
    "        if verbose and (count + 1) % 200 == 0:  # verbose=0时简化可视化输出\n",
    "            _now = datetime.now()\n",
    "            print(f'{_now.strftime(\"%Y-%m-%d %H:%M:%S\")}, {count+1} of {len(_train_loader)}')\n",
    "        count += 1\n",
    "\n",
    "        # 数据准备\n",
    "        stu_ids = batch['stu_id'].to(_device)                     # 学生ID\n",
    "        labels = batch['label'].to(_device).float()               # 响应真实值\n",
    "        if mode == 'fine-tune':\n",
    "            input_ids = batch['input_ids'].to(_device)            # tokenize内容\n",
    "            attention_mask = batch['attention_mask'].to(_device)  # 对应的mask\n",
    "            # 组装为bert模型输入格式\n",
    "            exer_in = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        else:\n",
    "            exer_in = batch['exer_id'].to(_device)                # 题目ID\n",
    "\n",
    "        # 梯度清零\n",
    "        _optimizer.zero_grad()\n",
    "\n",
    "        # 前向传播\n",
    "        output = _model(stu_ids, exer_in)\n",
    "        loss, loss_bce, loss_cl, loss_reg  = _model.get_loss(output, labels)\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        _optimizer.step()\n",
    "\n",
    "        # 累计损失\n",
    "        total_loss += loss.item()\n",
    "        pred_loss += loss_bce.item()\n",
    "        if mode in ['freeze','fine-tune']:\n",
    "            cl_loss += loss_cl.item()\n",
    "            reg_loss += loss_reg.item()\n",
    "\n",
    "    # 计算平均损失\n",
    "    avg_total_loss = total_loss / len(_train_loader)\n",
    "    avg_pred_loss = pred_loss / len(_train_loader)\n",
    "    avg_cl_loss = cl_loss / len(_train_loader)\n",
    "    avg_reg_loss = reg_loss / len(_train_loader)\n",
    "\n",
    "    return avg_total_loss, avg_pred_loss, avg_cl_loss, avg_reg_loss\n",
    "\n",
    "\n",
    "def val_or_test(_model, _data_loader, _device, mode='baseline', verbose=0):\n",
    "    \"\"\"\n",
    "    模型验证or测试函数\n",
    "    \"\"\"\n",
    "    _model.eval()\n",
    "    pred_loss = 0.0\n",
    "    # cl_loss 和 reg_loss 只在训练阶段有效，此处省去，因此total_loss也无意义\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in _data_loader:\n",
    "            if verbose and (count + 1) % 200 == 0:\n",
    "                _now = datetime.now()\n",
    "                print(f'{_now.strftime(\"%Y-%m-%d %H:%M:%S\")}, {count + 1} of {len(_data_loader)}')\n",
    "            count += 1\n",
    "\n",
    "            # 数据准备\n",
    "            stu_ids = batch['stu_id'].to(_device)                     # 学生ID\n",
    "            labels = batch['label'].to(_device).float()               # 响应真实值\n",
    "            if mode == 'fine-tune':\n",
    "                input_ids = batch['input_ids'].to(_device)            # tokenize内容\n",
    "                attention_mask = batch['attention_mask'].to(_device)  # 对应的mask\n",
    "                # 组装为bert模型输入格式\n",
    "                exer_in = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "            else:\n",
    "                exer_in = batch['exer_id'].to(_device)                # 题目ID\n",
    "\n",
    "            # 前向传播\n",
    "            output = _model(stu_ids, exer_in)\n",
    "            _, loss_bce, _, _ = _model.get_loss(output, labels)\n",
    "\n",
    "            # 记录结果\n",
    "            pred_loss += loss_bce.item()\n",
    "            preds = output[0].detach().cpu().numpy()                  # 获取预测概率\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())                   # 获取真实值\n",
    "\n",
    "    # 计算指标\n",
    "    avg_pred_loss = pred_loss / len(_data_loader)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # 二值化预测结果\n",
    "    binary_preds = (all_preds >= 0.5).astype(int)\n",
    "    acc = accuracy_score(all_labels, binary_preds)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "    return avg_pred_loss, acc, auc, all_preds, all_labels\n",
    "\n",
    "\n",
    "def my_gridsearch(_args):\n",
    "    \"\"\"\n",
    "    自定义点搜索函数\n",
    "    \"\"\"\n",
    "    # 定义参数网格\n",
    "    param_grid = _args.param_grid\n",
    "    # 生成参数组合\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    # 声明最佳组合结果存储变量\n",
    "    best_metrics = {'val_loss': float('inf')}\n",
    "    best_params = {}\n",
    "\n",
    "    # 遍历所有参数组合\n",
    "    for i, params in enumerate(param_combinations):\n",
    "        print(f\"\\n=== 正在训练参数组合 {i + 1}/{len(param_combinations)} ===\")\n",
    "        print(\"当前参数:\", json.dumps(params, indent=2))\n",
    "\n",
    "        # 修改待调参的参数\n",
    "        _args.tau = params['tau']\n",
    "        _args.lambda_reg = params['lambda_reg']\n",
    "        _args.lambda_cl = params['lambda_cl']\n",
    "\n",
    "        # 为当前参数组合创建独立目录\n",
    "        param_hash = hash(frozenset(params.items()))\n",
    "        _args.checkpoint_dir = f\"../checkpoints/{_args.proj_name}/grid_{param_hash}\"\n",
    "        Path(_args.checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # 运行训练流程\n",
    "        current_metrics = main(_args)\n",
    "\n",
    "        # 更新最佳结果\n",
    "        if current_metrics['val_loss'] < best_metrics['val_loss']:\n",
    "            best_metrics = current_metrics\n",
    "            best_params = params.copy()\n",
    "\n",
    "    # 输出最终结果\n",
    "    print(\"\\n=== 网格搜索完成 ===\")\n",
    "    print(f\"最佳参数组合: {json.dumps(best_params, indent=2)}\")\n",
    "    print(f\"对应验证指标: loss={best_metrics['val_loss']:.4f}, acc={best_metrics['val_acc']:.4f}, auc={best_metrics['val_auc']:.4f}\")\n",
    "    return best_params, best_metrics\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"\n",
    "    主函数\n",
    "    \"\"\"\n",
    "    # 自动设备选择\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 数据路径配置\n",
    "    data_root = f'../data/{args.data}/{args.scenario}'\n",
    "    train_path = f'{data_root}/train.json'\n",
    "    val_path = f'{data_root}/val.json'\n",
    "    test_path = f'{data_root}/test.json'\n",
    "    exer_embeds_path = f'{data_root}/exer_embeds_bert.npy'\n",
    "    exer_tokens_path = f'{data_root}/exer_tokens.json'\n",
    "\n",
    "    # 读取数据配置\n",
    "    with open(f'{data_root}/config.txt') as i_f:\n",
    "        i_f.readline()\n",
    "        student_n, exer_n, knowledge_n = list(map(eval, i_f.readline().split(',')))\n",
    "\n",
    "    # 创建检查点目录（默认为 f'./checkpoints/{proj_name}'）\n",
    "    os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
    "    best_model_path = f'{args.checkpoint_dir}/best_model.pt'\n",
    "    last_checkpoint_path = f'{args.checkpoint_dir}/last_checkpoint.pt'\n",
    "\n",
    "    # 初始化训练状态\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0.0\n",
    "    best_val_auc = 0.0\n",
    "    early_stop_counter = 0\n",
    "    start_epoch = 0\n",
    "\n",
    "    # 加载模型\n",
    "    dict_token = exer_tokens_path\n",
    "    model = MyCDM_MLP_FFT(num_students=student_n,\n",
    "                            bert_model_name=args.bert_path,\n",
    "                            tau=args.tau,\n",
    "                            lambda_reg=args.lambda_reg,\n",
    "                            lambda_cl=args.lambda_cl,\n",
    "                            ).to(device)\n",
    "\n",
    "    # 设置优化器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    # 设置Dataloader\n",
    "    train_loader = MyDataloader(\n",
    "        batch_size=args.bs,\n",
    "        id_to_token=dict_token,  # None or path( of json)\n",
    "        data_set=train_path,\n",
    "        offset=0,  # 使用原始ID的数据集\n",
    "        pid_zero=0\n",
    "    )\n",
    "    val_loader = MyDataloader(\n",
    "        batch_size=args.bs,\n",
    "        id_to_token=dict_token,\n",
    "        data_set=val_path,\n",
    "        offset=0,\n",
    "        pid_zero=0\n",
    "    )\n",
    "    test_loader = MyDataloader(\n",
    "        batch_size=args.bs,\n",
    "        id_to_token=dict_token,\n",
    "        data_set=test_path,\n",
    "        offset=0,\n",
    "        pid_zero=0\n",
    "    )\n",
    "\n",
    "    # 断点续训检查\n",
    "    if os.path.exists(last_checkpoint_path):\n",
    "        # 加载模型和优化器\n",
    "        checkpoint = torch.load(last_checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        # 覆盖相应的训练状态记录参数，打印\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        early_stop_counter = checkpoint['early_stop_counter']\n",
    "        print(f\"加载检查点：从epoch {start_epoch}恢复训练，当前最佳val_loss={best_val_loss:.4f}\")\n",
    "\n",
    "    # 初始化wandb\n",
    "    wandb.init(\n",
    "        project=args.proj_name,\n",
    "        config={**vars(args)},\n",
    "        resume=True if start_epoch > 0 else False,\n",
    "        reinit=True if args.grid_search else False  # 是否允许重复初始化\n",
    "    )\n",
    "\n",
    "    # 训练循环\n",
    "    for epoch in range(args.epoch):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{args.epoch}:\")\n",
    "\n",
    "        now = datetime.now()\n",
    "        print(now.strftime(\"%Y-%m-%d %H:%M:%S\"), f', training epoch {epoch + 1}')\n",
    "        train_total_loss, train_pred_loss, train_cl_loss, train_reg_loss = train(\n",
    "            model, train_loader, optimizer, device, mode=args.mode, verbose=args.verbose)\n",
    "        print(\n",
    "            f\"  Train Pred Loss: {train_pred_loss:.4f}, total Loss: {train_total_loss:.4f}, CL Loss: {train_cl_loss:.4f}, Reg Loss: {train_reg_loss:.4f} \")\n",
    "\n",
    "        now = datetime.now()\n",
    "        print(f'{now.strftime(\"%Y-%m-%d %H:%M:%S\")}, validating epoch {epoch + 1}')\n",
    "        val_pred_loss, val_acc, val_auc, _, _ = val_or_test(model, val_loader, device, mode=args.mode, verbose=args.verbose)\n",
    "        print(f\"  Val Pred Loss: {val_pred_loss:.4f} Acc: {val_acc:.4f} AUC: {val_auc:.4f}\")\n",
    "\n",
    "        now = datetime.now()\n",
    "        print(f'{now.strftime(\"%Y-%m-%d %H:%M:%S\")}, testing epoch {epoch + 1}')\n",
    "        test_pred_loss, test_acc, test_auc, _, _ = val_or_test(model, test_loader, device, mode=args.mode, verbose=args.verbose)\n",
    "        print(f\"  Test Pred Loss: {test_pred_loss:.4f} Acc: {test_acc:.4f} AUC: {test_auc:.4f}\")\n",
    "\n",
    "        # 早停逻辑（改为AUC优先）\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_val_acc = val_acc\n",
    "            best_val_loss = val_pred_loss\n",
    "            early_stop_counter = 0\n",
    "            # 保存最佳模型\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state': model.state_dict(),\n",
    "                'optimizer_state': optimizer.state_dict(),\n",
    "                'best_val_auc': best_val_auc\n",
    "            }, best_model_path)\n",
    "            print(f\"发现新最佳模型，val_auc={best_val_auc:.4f}，已保存至{best_model_path}\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        # 保存最新检查点（用于断点续训）\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state': model.state_dict(),\n",
    "            'optimizer_state': optimizer.state_dict(),\n",
    "            'best_val_auc': best_val_auc,\n",
    "            'early_stop_counter': early_stop_counter\n",
    "        }, last_checkpoint_path)\n",
    "\n",
    "        # 记录训练和验证指标\n",
    "        wandb.log({\n",
    "            \"train/total_loss\": train_total_loss,\n",
    "            \"train/pred_loss\": train_pred_loss,\n",
    "            \"train/cl_loss\": train_cl_loss,\n",
    "            \"train/reg_loss\": train_reg_loss,\n",
    "            \"val/pred_loss\": val_pred_loss,\n",
    "            \"val/acc\": val_acc,\n",
    "            \"val/auc\": val_auc,\n",
    "            \"epoch\": epoch + 1,\n",
    "            # \"val/best_loss\": best_val_loss,\n",
    "            # \"early_stop_counter\": early_stop_counter\n",
    "        })\n",
    "\n",
    "        # 早停判断\n",
    "        if early_stop_counter >= args.early_stop_patience:\n",
    "            print(f\"\\n早停触发！连续{args.early_stop_patience}个epoch验证集无改进\")\n",
    "            break\n",
    "\n",
    "    # 最终测试（加载最佳模型）\n",
    "    if os.path.exists(best_model_path):\n",
    "        print(\"\\n加载最佳模型进行测试...\")\n",
    "        model.load_state_dict(torch.load(best_model_path)['model_state'])\n",
    "\n",
    "    print('========================================================================')\n",
    "    now = datetime.now()\n",
    "    print(f'{now.strftime(\"%Y-%m-%d %H:%M:%S\")}, testing...')\n",
    "    test_pred_loss, test_acc, test_auc, y_pred, y_true = val_or_test(model, test_loader, device, mode=args.mode,\n",
    "                                                                     verbose=args.verbose)\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"  Test Pred Loss: {test_pred_loss:.4f} Acc: {test_acc:.4f} AUC: {test_auc:.4f}\")\n",
    "\n",
    "    now = datetime.now()\n",
    "    print(f'{now.strftime(\"%Y-%m-%d %H:%M:%S\")}, finish.')\n",
    "    print('========================================================================')\n",
    "\n",
    "    # 保存真实值和预报值\n",
    "    pass\n",
    "\n",
    "    # 记录测试结果\n",
    "    wandb.log({\n",
    "        \"test/pred_loss\": test_pred_loss,\n",
    "        \"test/acc\": test_acc,\n",
    "        \"test/auc\": test_auc\n",
    "    })\n",
    "\n",
    "    # 记录最终结果\n",
    "    wandb.finish()\n",
    "\n",
    "    # 返回验证集指标用于网格搜索比较\n",
    "    return {\n",
    "        'val_loss': best_val_loss,\n",
    "        'val_acc': best_val_acc,\n",
    "        'val_auc': best_val_auc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from deepspeed.ops.adam import DeepSpeedCPUAdam\n",
    "\n",
    "def main(args):\n",
    "    # 初始化accelerator（核心修改）\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=args.mixed_precision,  # 从参数读取或固定为'fp16'\n",
    "        gradient_accumulation_steps=args.grad_accum_steps,\n",
    "        deepspeed_plugin={\n",
    "            \"zero_stage\": 2,\n",
    "            \"offload_optimizer_device\": \"cpu\",\n",
    "            \"offload_param_device\": \"none\",\n",
    "            \"zero_force_ds_cpu_optimizer\": False\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # 设置随机种子（确保多卡一致性）\n",
    "    set_seed(args.seed)\n",
    "    \n",
    "    # 设备由accelerator自动管理\n",
    "    device = accelerator.device\n",
    "\n",
    "    # ... [原有数据路径配置代码保持不变] ...\n",
    "\n",
    "    # 加载模型（注意先不执行to(device)）\n",
    "    model = MyCDM_MLP_FFT(\n",
    "        num_students=student_n,\n",
    "        bert_model_name=args.bert_path,\n",
    "        tau=args.tau,\n",
    "        lambda_reg=args.lambda_reg,\n",
    "        lambda_cl=args.lambda_cl\n",
    "    )\n",
    "\n",
    "    # 优化器改为DeepSpeed兼容版本\n",
    "    optimizer = DeepSpeedCPUAdam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    # 数据加载器（保持原有逻辑）\n",
    "    train_loader = MyDataloader(...)\n",
    "    val_loader = MyDataloader(...)\n",
    "    test_loader = MyDataloader(...)\n",
    "\n",
    "    # 使用accelerator准备组件（关键修改）\n",
    "    model, optimizer, train_loader, val_loader, test_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader, test_loader\n",
    "    )\n",
    "\n",
    "    # 断点续训逻辑修改\n",
    "    if os.path.exists(last_checkpoint_path):\n",
    "        checkpoint = torch.load(last_checkpoint_path, map_location='cpu')\n",
    "        accelerator.unwrap_model(model).load_state_dict(checkpoint['model_state'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        # ... [其他状态恢复逻辑保持不变] ...\n",
    "\n",
    "    # 只在主进程初始化wandb\n",
    "    if accelerator.is_main_process:\n",
    "        wandb.init(...)\n",
    "\n",
    "    # 训练循环改造\n",
    "    for epoch in range(start_epoch, args.epoch):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            with accelerator.accumulate(model):\n",
    "                # 数据已自动分配到对应设备\n",
    "                stu_ids = batch[\"stu_ids\"]\n",
    "                exer_in = {\n",
    "                    \"input_ids\": batch[\"input_ids\"],\n",
    "                    \"attention_mask\": batch[\"attention_mask\"]\n",
    "                }\n",
    "                labels = batch[\"labels\"]\n",
    "\n",
    "                outputs = model(stu_ids, exer_in)\n",
    "                loss, pred_loss, cl_loss, reg_loss = model.get_loss(outputs, labels)\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "                \n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 多卡间聚合损失\n",
    "                total_loss += accelerator.gather(loss.detach()).mean().item()\n",
    "\n",
    "        # 只在主进程打印和验证\n",
    "        if accelerator.is_main_process:\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # 验证和测试需要封装到函数中\n",
    "            val_pred_loss, val_acc, val_auc = evaluate(\n",
    "                accelerator, model, val_loader, \"val\"\n",
    "            )\n",
    "            test_pred_loss, test_acc, test_auc = evaluate(\n",
    "                accelerator, model, test_loader, \"test\"\n",
    "            )\n",
    "\n",
    "            # 早停和保存逻辑\n",
    "            if val_auc > best_val_auc:\n",
    "                # 保存模型使用accelerator接口\n",
    "                accelerator.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state': accelerator.unwrap_model(model).state_dict(),\n",
    "                    'optimizer_state': optimizer.state_dict(),\n",
    "                    'best_val_auc': val_auc\n",
    "                }, best_model_path)\n",
    "\n",
    "            # 保存检查点\n",
    "            accelerator.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state': accelerator.unwrap_model(model).state_dict(),\n",
    "                'optimizer_state': optimizer.state_dict(),\n",
    "                'best_val_auc': best_val_auc,\n",
    "                'early_stop_counter': early_stop_counter\n",
    "            }, last_checkpoint_path)\n",
    "\n",
    "            # wandb日志记录\n",
    "            wandb.log(...)\n",
    "\n",
    "    # 最终测试（主进程执行）\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.load_state(best_model_path)\n",
    "        final_test_results = evaluate(accelerator, model, test_loader, \"final_test\")\n",
    "        wandb.log(final_test_results)\n",
    "        wandb.finish()\n",
    "\n",
    "    return ...\n",
    "\n",
    "# 新增评估函数\n",
    "def evaluate(accelerator, model, dataloader, mode):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            stu_ids = batch[\"stu_ids\"]\n",
    "            exer_in = {\n",
    "                \"input_ids\": batch[\"input_ids\"],\n",
    "                \"attention_mask\": batch[\"attention_mask\"]\n",
    "            }\n",
    "            labels = batch[\"labels\"]\n",
    "            \n",
    "            outputs = model(stu_ids, exer_in)\n",
    "            preds = outputs[0]\n",
    "            \n",
    "        # 收集所有设备的预测结果\n",
    "        all_preds.append(accelerator.gather(preds))\n",
    "        all_labels.append(accelerator.gather(labels))\n",
    "    \n",
    "    # 合并结果并计算指标\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    \n",
    "    # 只在主进程计算指标\n",
    "    if accelerator.is_main_process:\n",
    "        acc = compute_accuracy(all_preds, all_labels)\n",
    "        auc = compute_auc(all_preds, all_labels)\n",
    "        loss = F.binary_cross_entropy(all_preds, all_labels)\n",
    "        return loss.item(), acc, auc\n",
    "    else:\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_parallel(_accelerator, _model, _train_loader, _optimizer):  # , _device, verbose=0\n",
    "    \"\"\"\n",
    "    并行训练函数\n",
    "    \"\"\"\n",
    "    # device、模型状态、初始化loss计数、进度条\n",
    "    _model.train()\n",
    "    total_loss = 0.0\n",
    "    pred_loss = 0.0\n",
    "    cl_loss = 0.0\n",
    "    reg_loss = 0.0\n",
    "    progress_bar = tqdm(_train_loader, desc=\"Training\", disable=not _accelerator.is_local_main_process)\n",
    "    _device = _accelerator.device\n",
    "\n",
    "    # count = 0\n",
    "    for batch in progress_bar:\n",
    "        # # 进度可视化（改为使用进度条）\n",
    "        # if _accelerator.is_main_process and verbose and (count + 1) % 200 == 0:\n",
    "        #     _now = datetime.now()\n",
    "        #     print(f'{_now.strftime(\"%Y-%m-%d %H:%M:%S\")}, {count+1} of {len(_train_loader)}')\n",
    "        # count += 1\n",
    "\n",
    "        # 处理batch数据，组装为bert模型输入格式\n",
    "        stu_ids = batch['stu_id'].to(_device)                 # 学生ID\n",
    "        labels = batch['label'].to(_device).float()           # 响应真实值\n",
    "        input_ids = batch['input_ids'].to(_device)            # tokenize内容\n",
    "        attention_mask = batch['attention_mask'].to(_device)  # 对应的mask\n",
    "        exer_in = {'input_ids': input_ids, 'attention_mask': attention_mask}  # 组装为bert模型输入格式\n",
    "\n",
    "        _optimizer.zero_grad()                                # 梯度清零\n",
    "        output = _model(stu_ids, exer_in)                     # 前向传播\n",
    "        loss, loss_bce, loss_cl, loss_reg  = _model.get_loss(output, labels)\n",
    "        _accelerator.backward(loss)                           # 反向传播\n",
    "        _optimizer.step()                                     # 优化器调整权重\n",
    "\n",
    "        total_loss += loss.item()                             # 累计损失\n",
    "        pred_loss += loss_bce.item()\n",
    "        cl_loss += loss_cl.item()\n",
    "        reg_loss += loss_reg.item()\n",
    "        progress_bar.set_postfix(\n",
    "            loss=total_loss / (progress_bar.n + 1),           # 进度条可视化\n",
    "            bce=pred_loss / (progress_bar.n + 1),\n",
    "            cl=cl_loss / (progress_bar.n + 1),\n",
    "            reg=reg_loss / (progress_bar.n + 1)\n",
    "        )\n",
    "\n",
    "    # 计算该轮训练的最终平均损失\n",
    "    avg_total_loss = total_loss / len(_train_loader)\n",
    "    avg_pred_loss = pred_loss / len(_train_loader)\n",
    "    avg_cl_loss = cl_loss / len(_train_loader)\n",
    "    avg_reg_loss = reg_loss / len(_train_loader)\n",
    "\n",
    "    # 返回各项损失\n",
    "    return avg_total_loss, avg_pred_loss, avg_cl_loss, avg_reg_loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
